# mamba_vlm
è¿™ä¸ªé¡¹ç›®æ˜¯åŸºäº [Mamba-VLM](https://github.com/OpenGVLab/MambaVLM) çš„å¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬å›¾åƒé—®ç­”ä¸æŠ¥å‘Šç”ŸæˆåŠŸèƒ½ã€‚

---

```markdown
# Mamba-VLM

ğŸš€ An open-source multi-modal visual-language model based on [Mamba](https://arxiv.org/abs/2312.00752).  
Supports image captioning, visual question answering (VQA), and image-based report generation.

## ğŸ”§ Features

- ğŸ” Visual Question Answering (VQA)
- ğŸ“ Medical or Remote-Sensing Report Generation
- ğŸ§  Efficient inference using Mamba structured state-space model
- ğŸ“¦ Modular & extensible architecture

## ğŸ–¼ Example

```bash
# Example: Ask a question about an image
python inference.py --image_path ./example.jpg --question "What is the main object in this image?"
```

## ğŸ“ Project Structure

```
mamba_vlm/
â”œâ”€â”€ models/            # Model architecture (Mamba encoder/decoder, vision encoder)
â”œâ”€â”€ data/              # Dataset loading and preprocessing
â”œâ”€â”€ scripts/           # Training / inference scripts
â”œâ”€â”€ utils/             # Utility functions
â”œâ”€â”€ checkpoints/       # Pretrained or fine-tuned weights
â”œâ”€â”€ inference.py       # Main entrypoint for demo
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Clone the repo

```bash
git clone https://github.com/lijin6/mamba_vlm.git
cd mamba_vlm
```

### 2. Create virtual environment and install dependencies

```bash
conda create -n mamba_vlm python=3.10 -y
conda activate mamba_vlm
pip install -r requirements.txt
```

### 3. Download or prepare model checkpoints

Put your model checkpoint (`.pt` or `.pth`) under `checkpoints/`.

### 4. Run Demo

```bash
python inference.py --image_path ./example.jpg --question "Describe the image."
```

## ğŸ“Š Datasets (Optional)

- [VQA v2](https://visualqa.org/)
- [MIMIC-CXR](https://physionet.org/content/mimic-cxr/2.0.0/)
- [RSICD](https://github.com/ucas-vg/RSICD_opt)

## ğŸ§  Model Details

The project combines:

- ğŸ”µ **Mamba**: A linear-time state-space model for long-range sequence modeling.
- ğŸŸ  **Vision Encoder**: Supports CLIP, BLIP, or ViT for visual feature extraction.
- ğŸ”´ **Language Decoder**: Based on Mamba or LLaMA decoder blocks.

## ğŸ’¡ TODO

- [ ] Upload pre-trained checkpoints
- [ ] Add training instructions
- [ ] Release web demo (Gradio / Flask)

## ğŸ“„ License

MIT License. See `LICENSE` for details.

---

### ğŸ‘¤ Author

Made with â¤ï¸ by [lijin6](https://github.com/lijin6)
```
